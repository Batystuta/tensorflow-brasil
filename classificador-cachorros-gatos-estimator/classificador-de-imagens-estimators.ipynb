{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificador de Imagens usando TensorFlow Estimators\n",
    "\n",
    "Neste notebook você irá implementadar um modelo para classificação de imagens, classificação é uma das aplicações de Machine Learning em que o ensino é **supervisionado**, em outras palavras nós vamos ensinar ao modelo através de exemplos em que já sabemos qual o resultado esperado.\n",
    "\n",
    "Nosso modelo deverá receber imagens de gatos e cachorros e identificar a que **classe**(gato, cachorro) estas imagens pertencem.\n",
    "\n",
    "## Dados\n",
    "\n",
    "Os dados foram retirados da base de dados [CIFAR 10](https://www.cs.toronto.edu/~kriz/cifar.html) que contém 10000 imagens de 10 classes distintias, para este exemplo iremos utilizar apenas as classes gato e cachorro.\n",
    "\n",
    "## Modelo\n",
    "\n",
    "Iremos utilizar diferentes modelos com diferentes níveis de complexidade.\n",
    "\n",
    "\n",
    "## Créditos\n",
    "\n",
    "Essa atividade é baseada no notebook encontrado [aqui](https://github.com/random-forests/tensorflow-workshop/tree/master/extras/cat_dog_estimator) implementada originalmente por [@chrisying](https://github.com/chrisying). Obrigada a todos os envolvidos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibilidade entre Python 2 e Python 3\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Bibliotecas auxiliares\n",
    "import cPickle  # maior eficiência ao processar as imagens\n",
    "import numpy as np  # manipular vetores\n",
    "from PIL import Image  # lidar com imagens\n",
    "import matplotlib.pyplot as plt  # plotar imagens\n",
    "%matplotlib inline\n",
    "\n",
    "# IMPORTANTE: essa linha garante que os números gerados aleatoriamente são previsíveis\n",
    "np.random.seed(0)\n",
    "\n",
    "print ('Sua versão do TensorFlow:', tf.__version__)\n",
    "print ('Recomenda-se para esta atividade uma versão >= 1.4.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscando os Dados\n",
    "\n",
    "### Baixa e estraia os dados\n",
    "\n",
    "Para baixar os dados, execute os seguintes comandos na pasta em que se encontra este notebook.\n",
    "\n",
    "```bash\n",
    "curl -O http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "tar xvf cifar-10-python.tar.gz\n",
    "```\n",
    "\n",
    "Ou você pode baixar os dados manualmente [clicando neste link](http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz) e extraí-los nesta pasta.\n",
    "\n",
    "Após extrair as pastas você deverá ver as seguintes subpastas:\n",
    "\n",
    "```bash\n",
    "cifar-10-batches-py/\n",
    "cifar-10-batches-py/data_batch_4\n",
    "cifar-10-batches-py/readme.html\n",
    "cifar-10-batches-py/test_batch\n",
    "cifar-10-batches-py/data_batch_3\n",
    "cifar-10-batches-py/batches.meta\n",
    "cifar-10-batches-py/data_batch_2\n",
    "cifar-10-batches-py/data_batch_5\n",
    "cifar-10-batches-py/data_batch_1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando as imagens no formato .npy\n",
    "\n",
    "Para facilitar a manipulação das imagens iremos salvá-las como vetores utilizando a [biblioteca Numpy](http://www.numpy.org/) que é muito fácil de se trabalhar além de ser altamente compatível com o TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARQUIVOS_TREINO = ['cifar-10-batches-py/data_batch_%d' % i for i in range(1,6)]\n",
    "ARQUIVOS_TESTE = ['cifar-10-batches-py/test_batch']\n",
    "\n",
    "NUM_DADOS_TREINO = 10000\n",
    "NUM_DADOS_TESTE = 2000\n",
    "\n",
    "LABEL_INPUT_GATO = 3\n",
    "LABEL_INPUT_CACHORRO = 5\n",
    "\n",
    "LABEL_OUTPUT_GATO = 1\n",
    "LABEL_OUTPUT_CACHORRO = 0\n",
    "\n",
    "def unpickle(file):\n",
    "    '''Essa função apenas torna mais eficiente a manipulação dos dados, não se preocupe em entendê-la a fundo.'''\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "        return dict\n",
    "    \n",
    "def converter_para_numpy(arquivos, num_dados, saida_numpy):\n",
    "    '''Essa função converte os arquivos de treino e teste em arquivos .npy'''\n",
    "    \n",
    "    # Recupera cada dado (imagem, label) nos arquivos\n",
    "    dados = [unpickle(a) for a in arquivos]\n",
    "\n",
    "    # Salva as imagens e labels em arrays numpy\n",
    "    imagens = np.empty((num_dados, 32, 32, 3), dtype=np.uint8)\n",
    "    labels = np.empty((num_dados), dtype=np.uint8)\n",
    "\n",
    "    index = 0\n",
    "    for d in dados:\n",
    "        for batch_index, label in enumerate(d['labels']):\n",
    "            # Caso a imagem seja classificada como gato ou cachorro a consideramos\n",
    "            if label == LABEL_INPUT_GATO or label == LABEL_INPUT_CACHORRO:\n",
    "                # Os dados originais são armazenados no formato 1 x 3072 , convertemos para 32 x 32 x 3\n",
    "                imagens[index, :, :, :] = np.transpose(\n",
    "                  np.reshape(d['data'][batch_index, :],\n",
    "                  newshape=(3, 32, 32)),\n",
    "                  axes=(1, 2, 0))\n",
    "                # Salvamos o label de saída correto\n",
    "                if label == LABEL_INPUT_GATO:\n",
    "                    labels[index] = LABEL_OUTPUT_GATO\n",
    "                else:\n",
    "                    labels[index] = LABEL_OUTPUT_CACHORRO\n",
    "                index += 1\n",
    "\n",
    "    # Salvamos no arquivo de saída\n",
    "    np.save(saida_numpy, {'imagens': imagens, 'labels': labels})\n",
    "\n",
    "# Convertemos os arquivos de treino\n",
    "converter_para_numpy(ARQUIVOS_TREINO, NUM_DADOS_TREINO, 'treino.npy')\n",
    "# Convertemos os arquivos de teste\n",
    "converter_para_numpy(ARQUIVOS_TESTE, NUM_DADOS_TESTE, 'teste.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando se dados foram salvos corretamente\n",
    "\n",
    "Após manipular ou modificar os dados é sempre importante garantir que os dados estão no formato esperado e não foram corrompidos ou alterados indevidamente. Para isto vamos escolher algumas imagens do conjunto de treino aleatoriamente e verificá-las.\n",
    "\n",
    "> IMPORTANTE: para modelos reais é importante garantir a qualidade e integridade dos dados com maior rigor já que é fundamental a \"saúde\" dos dados para se obter um bom modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_treino = np.load('treino.npy').item()\n",
    "# NÃO DEVEMOS OLHAR/MEXER NOS DADOS DE TESTE. SÓ UTILIZAREMOS ESTES DADOS PARA VALIDAR NOSSO\n",
    "# MODELO DEPOIS DO TREINO.\n",
    "dados_teste = np.load('teste.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato das imagens de treino: (10000, 32, 32, 3)\n",
      "Formato das labels de treino: (10000,)\n",
      "--------------------------------------------------\n",
      "Formato das imagens de teste: (2000, 32, 32, 3)\n",
      "Formato das labels de teste: (2000,)\n"
     ]
    }
   ],
   "source": [
    "print ('Formato das imagens de treino:', dados_treino['imagens'].shape)\n",
    "print ('Formato das labels de treino:', dados_treino['labels'].shape)\n",
    "print ('-' * 50)\n",
    "print ('Formato das imagens de teste:', dados_teste['imagens'].shape)\n",
    "print ('Formato das labels de teste:', dados_teste['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplos de 5 imagens de teste\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABpCAYAAAAqXNiiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztvVeTJVt6HbbSHu/KV7Xvvn39HT8cYDgGHAiiGKRIgSEJ\nCgYe9CqFnvRERfBBoWDoF4ihCIaCAQkPegMlkqBECYaAyCCAcdebvu27fNXxJk96PXzry1PVdzTo\n06D63inu9XK66+TJ3Llz595rf2Z9Vp7nMDAwMDD4xYf9eTfAwMDAwODfDsyEbmBgYHBBYCZ0AwMD\ngwsCM6EbGBgYXBCYCd3AwMDggsBM6AYGBgYXBGZCNzAwMLgguLATumVZK5Zl/WPLsqaWZT2yLOvv\nfN5t+jxhWdZ/ZVnWjyzLCi3L+q3Puz1fFJhx8lmYsfKz8YswVtzPuwH/P+IfAIgAbAL4CoDftSzr\nnTzPP/h8m/W5YR/A3wfwVwFUPue2fJFgxslnYcbKz8YXfqxYFzFT1LKsGoA+gDfzPL/Dv/02gL08\nz//u59q4zxmWZf19AJfzPP/PP++2fN4w4+Tnw4yVBX5RxspFNbm8DCDRjifeAfDG59Qegy8mzDgx\neFb8QoyVizqh1wGMnvrbEEDjc2iLwRcXZpwYPCt+IcbKRZ3QJwCaT/2tCWD8ObTF4IsLM04MnhW/\nEGPlok7odwC4lmXdPvO3LwP4wjgvDL4QMOPE4FnxCzFWLuSEnuf5FMDvAPjvLMuqWZb1lwH8LQC/\n/fm27PODZVmuZVllAA4Ax7KssmVZFznK6c+FGSc/G2asfBa/KGPlQk7oxH8JCbk6BvC/Avgvvkjh\nRZ8D/h6AAMDfBfCb/Pff+1xb9MWAGSefhRkrPxtf+LFyIcMWDQwMDP5dxEVm6AYGBgb/TsFM6AYG\nBgYXBGZCNzAwMLggMBO6gYGBwQWBmdANDAwMLgheaGzpDz+5lwOAPZcM2mxwgNMnIo3gjPYAAK9d\n8gAAYZwCAGaxDdsuAQAG3T4A4PikBwDwfGl+nlpybDBFFM4BAMFEPufzCQDAsiWap1aro72+CQBY\n2dzi7xMAQBrL52g4wDyOAACVWhUAMBlJm/v9oZzP9+V8zQ5qNRGk831p+9/+r/9H61n75Df+m+/m\nADCzpgCA1WYTiRtLH8zkmDSQT6/EH5VzNMM1AECLfTCoye/v7T0CAGSMXnrp2iX0R9JmL5N7yR05\n/9w/kf+PfYyP5d6DlN8NMrmULffZ7QbYKncAAJ0Vud8noyMAwNWNHQDAD375VwAAr13/Bt756A+k\nXQeHAID//r/9nWfuk9/8T/7jHACu335dzvtrv45avQ0AePn2NgCgUZPT5bF0TpbEyGz2WyR98eD+\nQwDA2po85zKf0+PHBxgOJcGvWpX7S0L5rWOX5RzzFHc+vgcAODqV+8xt6ZP19XX+tookkd9ZHKNx\nKn1cakhfeRU5f6PZQrkkWeK2LW3/937pxjP3CQD8D//Tb8lDzR0AQK22AsuVP2XWqVy3JMmMq+1L\nAADfreDoyS4AYHNT2h2kIQDg+FCejcf3B3YJk0jGfX8g79j9R9IHzVYLALDSXEW5JPdqSTOQcMy4\nnoxFBwASGU95LN8lUXLu2DiR60RRgMlU3q3hUK75j/7Rbz1zv6ysbuQAkOdpcX7bFp5qWRwjfBeK\ngL48h5NLe371m7ekb9ZXAQDzmcwX3aGMoZ/ePUFvPF/cFwDw/M2mjMlSqYIcT0UL8r8p2xXMptho\nSP+8eWMFAJBZNZ5Ozvz4WOa3e4dD5Bl/z348OT54pj4xDN3AwMDgguCFMvTVDVmZkqmwlkk0hW1L\nE6JI6OigL6vVYEy2leRo1WUlK5eEAa+tCftJAllFUzKzNCvBI3Mul+RzPJY1az6XY5MsRUpWlabC\nEiyuphlX0zgOi1XedR1+yrXLZWFwNtmI5ZaQcu223OW7M7d13RdmN3dCWKmcp1oje+TuQilCo2Uj\n6iqjlqU8CuVemg1hDb2RsO/d42OQWGLIPq615Ly6+idzH25J2GsykYNJIpGO5LylzEc3FCY1HUhf\nbqwJa2u15eCDA2GC8cTFOJBdwfb69WW7BIOh/Pbx44cAgHt3P4Fl1QEAk7GwmLdel11BoyLPybYz\nAGSFubLm7NwneE9/+Af/D8K5/O3LX3kLANA/FbYaR8qI+tjbFWaeWXK+zJbvYu4KPN/D0d6B/NuS\nPr189RUAwPG+7DhtX9jsrdfeQmOzxTt8Ph71wYcfSzsgY/Ctt+ooW3KuLBdmHs9kF+Bmct21Sx4e\nHEpbjvuyK2l15JjpcCC/nUh/H5xMMCVbX9vY5Hll3P/pD98DAFT9GrbI9Eslh/0hDLZSkrZ4joWE\n57Eyjl2+cwVDj2WXEAQTBHw3Z2THy8AqeKv8Q9/b88foQXlxbCavOiK2y/fk+WXcRW205ICv3lrH\nD+/I7mfK3YvDPnccuX/bcYp26C4gy2R8Zdzplz3g9VvSbzXu2sYTjl1XPmP2VQ4HvARsd6lNnGHo\nBgYGBhcFL5ShB1zhLK5sbq0BryRs4/GJsL/egazcuSeM1cszZLSn+xU5Nkl0pZX1aDyT86aZhRKP\ncWnT1NVZV9AginBIG/wkkvNWynKsRzY1DUKUqnJ9NRRajnSVz11CRkaQwkFKm2aaZEv3iVOT89qB\n/DZIhnBi+VsWyc6k5AlrmEXCqErzGmxP2jGnz8BmH2VkPnki7evOevBj6ZNaSc47mggTqjSkH4M4\nxmwo7LNpy7XCWNj8mPb3G5duYj+QfpsN5ZpTT67Vq8tvD+4Ji5tO3sbOJWEjKzdvLt0nGQ2I06kw\nyn7vBBwm+Kf/5J/Jd6MvAwC+++2vyL2UXSSZsj+OB7KvjHTsYP8YAPD22x/g9ksiY/3Rh/flmETG\n32wqffPpnUfIyDJ97gJyh34dMsmjoyPcv/8AAFAqybO6svtErp27bIucY//gGP/h3/qPAAA17qKW\nxXAsTNZy5Ll96xsvo1HmjvZUxsOsL98p63v91U2887a0pduV/mw0ZaegO1OPPpV2zULbl+ffaMuO\nqFx5+dx9PLz/EEdHsptZbcs7YoEsN5NrpgAiS75zbWmXS/ZpF+8j7dvICxv3gkk/D/S8Np7Ofl/Y\n0BdtSHl8xHc2TeTZVkvcKScyxm9eKmHGY96+K7teh7uATkv6aHNzE25Onx3fx+FUxmC3L5+Xt9dx\n9fI6j5F2pOqv4js7DeU9yvLFDnnZRP4XOqG3azLo56HcgNNaw9Yt2fKePvoIAHDv3Z8AAPKyOClK\nroeSz22lTtzcLvl0COWWDBrHzYsJzXHPb2c8OnKmYYwprz8K5AG5nKxrDRnMURBgxRcTRJbrVo4m\nFna0x98EaYrcknNn2rBl+qTDR+BzCxo6yDhxdOmIXW/LFs3xZGCFQYx2VWa4iBN5msvEO52JSSLN\npE+SyIKaIppVaWdAh1S/L+dzsxwTTtJ1VwapHdFUlXGxynPUIee0aebx+P9gIG24dflVAMCJ00P3\nRCaPx63dpfvELfv8l5x3PhthfeOy3Hssk2mciNkjiuiwsnykFvsilXGRFOYo+ftPfyILTqe9Wlzr\nQE0jlpyv15VFazgcADQ3OPJoYHty3vFY2rD75AkGE1n4wlwm23EiB+/sXAMA5Fyc7370Lj5+6QoA\n4I0vf51Xv7ZErwDI5ZlUy3JfVXsffvip/Bs0Gc3kXh1bnKL9e1/C8QNZtOaxPNsBJ3Sb/dRu0Fke\nj3BCB7Bew3fknVhvyXibdEo4OZJnmtAsemVbTKmuI/3VH0eIyzJ5kXeglKm5gotiwH6bhwj5Pj7P\nhP6049OCDTWtLKwx2blj8nwRNDCZybsRhTSjkdyVKjJXOW6EV26JIz7O5IzVivTXG1/7FgBxqIZD\nWcgDmoG7fJ/GM7n2zvYaKrbcZ5TIMR5NxVO2ZRacJyDy7+VIojG5GBgYGFwQvFCG3qQZo062nFSq\nAJnm+Lawu4cfvgMAOB4KCxqEDnpjroxk5JPpjOcTFl2lc6ZV9+GS1dmOfe5TQ41yOJiTHQR0JOZk\n4X2GJ1l5BseXtparuo2nA5GfFV/aHc/mKHHFfh6hs8J65JJNBtGC1TCkzq9zNxDI53jcB6Iy70GY\ncBRKf00m0l6UF45fP5It/pxb9ZiMZTziNrMcIgnkvp6Mhem5NDc4ZKkP90/oigMaNBMNh9KPJe6Q\n3rwizPOofBf/6v0fAgBOT/eX7pNyWZ6rmkzGoz6SVFjNtesbAICVVZqhZjMem8LxuOWXW0HO3cXR\nURcA8OC+sKharY7d3YcAgF5PGGlCh7GyzFKphMlYGFVMkuTRtObQnGfbJVgg8wR3T2Tss4Hsriqe\nOCtza4b33/kzOTd3Sr/2/S8v1S8hx2vZlWc9O/kR8vmPeV3Zbdo9YZrDofTPnfd/iMEjjuvmDQDA\n6aEc22ozzJBmyksbFUxOhM2PjzkeHWl/xN1xxQ1xaV2O5zDFdkfGyqAr/dzrBsiacky5It9lufze\npXlmOpXnOZ9PkTGEUAMQ/qJ42gn6tMkFllUcc9KXvkyvarvkuZVr3K1HCVa3ZbfxnY7sbDprslu8\n+dZflvNFfUScoyxXnu1kwnBaejejzEbvRPo9PZUxktEMNZjQ5DLTENwMibL0JacUw9ANDAwMLghe\nKEOfR3Se0ClaKZWQMxmnuS72xTK/i+dy7GkAPDyW1bxKp55DZ0KNSTW+Lytb3U2w1ZIV0qXHJ2MI\nm0emnueAlZHCpWqzYmLSRK6TWxZqDJVstmWlTtn2hA4Sn7atYDJDrS1hlNaSIUYAMBiL7XvEkMAo\niQtDfYmOpYzXbvnCFBynghFDB/s9YcA0+SOZO7xfuf9y1IJ1IAx9vCLhV0lF2FI4kX6wswRInMX1\nASRz7oo8MnUvxzSgM3vdZvvk/+O+MI7f/5N/AQCoN4HtlvTbp/eeLN0n7RbbO5bdx2Q0wGwqtu0r\nVyScrkpWqQ7KJArhM1Q1IwvUUNMPPmC4H3dilp3iwUOxPU/4zOtV2kxJcSplv9jVlcvqo5Hvygzx\nrNUbGDGBzSJDV6YeTKTtde4ksjzH4eER2/Pe0n0CADadvj069buDCsq5hG9yk4S0Kc80zKSNjdZV\nWA/UUSttO+nJLiwIuaOZym+utVOsb8hY3juVcTnsi0+m7MkzsWdHuLQtTHVrg8k4Iznm0VA+D08D\n2DpWPGHtdYY01mijngd0rM9mqPBZWrZy/r8YFkycc8BTdmjHcQrn7OmIzv+A4bncSUxn0teN1Rrq\nDdmlrHTkc/OKhKbuXLkOAAgHQN5i213unBlwMZ/TNzOcIJjLuRtNhovSbn/S57tPhyqw8Mctu+s3\nDN3AwMDgguCFMvSQYYs5mVPq2LBdRnBUueo3pUkzMtDD0MeDKZkVV/3NuqyCvius6oTJJvPBAKM1\nYSbXtpjEwcV5TBup77nwmBS0RmYQM2QpIOOfhxGSuRyvTCLStjMZxydrDudTpIne1/I2wP6JrNYB\nmXGllSO25JrlXOzFw0NGwLhigzvuzjFlpIVvM3SM7NNn++y+3Eu662M0E7ZYW2e41EyOjenhzwML\nc0a8aLKE7hLmM7n2yvoKKnVhVzkjicKytDOzpf8+OhLm+UrzBv7m174PAPj96CdL90mm96BJZ+EM\nJ0fCbrd3hJGqffzw6JjtHGNlVVjkxpbsZJptGQN37z/iLcnY6PVP0Weaucv+CyO19cpzyO0YmhFf\nYtRNEW1Fu+hKp41TMliVitDQ15i7v8TSHWIVQSR9/OjR8rsWALi0JRIGH38s13Rbfwn1znelbVVh\n1PFc2uoN5V63N27iD979BwCAWSzjQO3YY8ofNEnvg0qpSG6ZzuR8O1tXAQDXroiP6+3oCGVGZMWB\ntCNgGOzesYzlg24AZyDnfno81asSaVOvyWcUhbBpd3b+rcxGizDInMx8kSTIEGHbgRqnp1Ppi8fH\n0t6v3hL7+BHfy7bXQqMpY2R1TeQ2NjflvWxzFzpLakhoUUh4v2X6C6dMbJxNZ/A4P1Qbcu9j7s5n\nZO5ng3xUHmLJIBfD0A0MDAwuCl4oQ080tZVsN7EtVGiH1SiNtQ7FoPYonhQl8Hxh4t2BHFO35f9t\nCmeNA2EMs2mEqCV/0xjkkkboc/m3bA8eY0zbHdq+GacdR3KeLE2LhIOM5wnnTNihHXNOj3Qcx4gZ\nCx3by3dnpSXnm53I2tqur2IOxpLn8jkm+zv6VPrk5L0pOk1hC1evCXMaP5K+6Z8yTZnp3FmSoXVJ\n7jdmQsNsIJ+exlk7DjImNcSMdrEdxt/z+Xi2h5hSCRMKF1kRU58hz2ynLTbG/skQ/4JRLhZ3HstA\nU8EtV22rFoaMRphzV6EJPQ8eyK4giSa4cUPY5NblH8h90QcxGsnzGY3pQ0jnaDSEvWvzNOqi2Inl\nOSpkWZoIV60Ls9IY6nq9jPVVuefdfZEAyBn7kXOsB4GMk1pjDTF3dQGvtSy8sjDC5orYrh/sPkTp\nSNrY5y5On1ulKu3o9k8wHDPSyKHkg0YwZfKbakXO2+pUCtGy9orsdr71bYm1vv2SCKWdHLyPvV3p\n+96J/H5/V3Y7NnfMtaqDI+6cHLJSZckR3yN9186m6juMqPmLIM/PJBDx3KUir+EsuKOlVMj+iYyv\nb31Jxtytlowlr1ZBom3lzsxmLH2Wym8c20LIdyMjR04ph6CftpXDoYNG8yxoqECFonvaXmHlmmy1\n3P2/0AndZYMdttKxLfROxWmyty+TV7Mug7XqPwQATI+eoFOVrWbAbKwpnTjjmgyWKUPFOlUPa215\nIA3qJeScbD0mhXhlB+UakwfoCNOHqslIrudCJSgiSh5q+OOwL+0dMWsvTdPCRBBy67QMLDqmHJpB\nhr0ZmMuBmCamzqq0c/BAzt+KgemeOFLe3f2p3CdDv3Srb3FglUolJB6/Y9aa69OxmzLRKwphqyrc\nkNmzmwzN48A/PumByXPwGLb417/8bQDAh3fuse3MJLViHB/clWPT5TeB9Ra1eyoy6abzECqTM5ty\nUs5lS9xZl2d3aeeVxeLDMNbxiJMHF+fHu5JEtLLaRrsjZpnBgAvfjM5NTkBJlhZORF3cg1D7VgbH\n6koDK+3bvHdp16CYrJnQ5tFpiwwR4ynt56zjG8w5SfA8//P/8luoM7zu9HDKY+R+Vtc17NZCxacq\noC/vVrcvz+nmpRXej9zfYNgtVBAdSnu2Vqi/xPDM0+4BpgPpe4fEakYHeqUux16tlWBTpfPkhCax\nQMkSlU05Afq+j5QkqVpTOdFnx9PJSHmeF39TXafFRPlZ+4UqpA7o3L7zSBbmX/qqEKXMtmAVC4T8\nRpOH/In0I+IQkS7WnAtimmy1dVYew9NkRy66et9XtuT5DGn+6Y/Cn9vmnwdjcjEwMDC4IHihDP3R\ngaxoLUoA+I6Fd376IQCgdyCr3tcawsavXZctz+VHMzx6KKt8lck8M5og9k5kRStTjfBSp4YOkzYY\naQb6O4uVsuTZqHhqRpC/6UpebcqWz6/WQUsLkkAYVpnJQy5X2eMD2cZGUYRIt2TLejAAVMlC45qy\nyglyhjO5DKd0AmlXLRCH4P780yJEckqtFV3JNUmqQ/1qv+wjm0onlJzzDtThSK4TpiGSWc571y2y\n6o0zoWrmA8riR3LsAzr3KpQmOTxWc1SMelmun9mLUKxnxea2mJNCmnSCJEWpIG/yPF55VZxXW1ek\nbzbWVjHocgvM57u7LynqE1XlLNQ0c9Qp81CmiSXtC+vUv7uuWzBihWqfV3y5/831day15Pp3PhLz\nXfcBTS8ccCpNEIdhMQbT5xgnALC1QcXJkQzuTz4+RcRwu7VNCfsdj+QZz8j20qSG1maHv5P3TzW/\naxUZTys0lSXxDBmnBJdmzmpVvuuR1T9+9Bg+s+E092XIJJqtHblOs9opTFqXL0u79nblWeztPyiu\nBQBh6GJONtts1Jbuk0LVMFNlTRu+p2Gm6pB9Sg/9bLaOqiPymHfvyC7u2lUJj13r1DDnrutgV8I9\nbU/a2WrJjifPssKMpTuRw30JYBj2ZFx5pRx+U3YwanbyynIeDYfcWpF+nMxiJOnz6dsYhm5gYGBw\nQfBCGfr7H4qt9aUb1wEAwWSED5n0UWY6esBQwlWGBn39tQAPho8BAHfHqpYm1KBGFn6zIyvdjfU6\narSfphT8Ufu2z6Qkz3MKrfQSP/2KhHh5TBhxvUphj7XoCCnTAat29ojsbTh8hJD/divLr480p6HG\nZJ/xoYfuHq9ZlfOWPLH3YiL3PZlO4dCjou1Tu6o6hMqURXjt1bewSwXAqCd9MmcKOQvXIAqyQmvd\nKasQGncHFVaFcuxisHiWPKP9HhO+mGjUqnMHME+w2qBzm+nzy6BJxrK3z5DOcA5PBZa4dVpdEQa1\nukqfS60MCyX2hfTBT9+RtPjxVPwzG5vymzRx4Dj0EdDhqczcwsL+qqqPajuPGK5584pcc6VTRzSl\nzZrKexX6ZQKOv5hjIxhN0GDClPUzNLufBf/pr4uz9x/+QwnDbJfLOD4R232FCo5emcl23FG2O6s4\nPZLdZMjw1QYZecydoDL0br8LCzJu3nztmwCA9S2RCzjYE0mA61dvI2fI7PsfSHJWfyzP6Spt4O2V\nNXjUF99kVbC1NRnDOaQ/Htz/RNqb+0XyTX/QX7pPnlZqdH0fDsOSkZ9n5tZCrquoMKQiXSq21x3I\nbuG9j2Un8crlVbx/yrDftoyf7zvSR5eviJKoW64ip7b88ZHcw+lY+uLt94TVR5PH+Oa3v8p2UBSN\nQRm6TbhEca6jXoD+ONBGLwXD0A0MDAwuCF4oQ9cKPBUKUR0OjnH3U6kpurUmrOe0Kt9ttsXedP1G\nhrcOZNU8+UQiTDqr8t2VdVYXITNoVErwHdp8GS6WndFABgDXWSQVoLChy+8rNSYjuS5yVqDJmHSR\nUa61sSJSmlv0and7XeQ0Jmb58olFTsBdAu2I+XQOn9ded5jAkAkz2O+LvyFOYiR0Dmgo1KKOIm+B\nIZS/9oNfLULyfvf/+OcAgL19YewZI2KQ5choF41VLYzyucjkPImXFOxdayAykGFRV5MRLd/51jew\nsSIM9Z/9/jtL90mDtlTfm7J5FgLaio/2xZb7p/9KznuViSA3b11FMJPInxHv98c/kaSmIYXe3nzz\nJQDAdBIjZq3LgIlTDTL0ROWIs7QIWww0ZJWRGRWy30q5hhmZeI1tbvMzV3kJPqf5dIRmQ5idY/+s\nMLo/H48+/BEA4MkdYbcVx0KHdVJH1DoHd5Br6yppG+DhY3nHOmTxK4587lwRid31bRnTuRfjS1/5\nJQDANYbDJqzPqjvov/Mbv4l//Nu/LdekAFmJVbw0iahaqRRRYiqNq/17+aow/gpD91ZbNfT7sstY\nOkYPZ8IeVdbaX8gH6Lt/ppSo/qtg6EUUioYkcuf76UNp02wwhs0d6WosfdE7ERZu8VinVCps6F3a\nzsvt6wCAOTvi6HSKyVje8Z3L8j5Hico8y1jZ2pYxuXE0LvwS2Rlz/7PAMHQDAwODC4IXytBfuils\nKqCIT5LFSCl3qzUdj2gDv03Bq2qrieuXhL1vP5HfVRmecntTmIYyOsuxkAWaTivXdBkNorG2WZIg\niZ8S1Nc0W7bT9UoAJVI9CgYlMVkpk1VI1LF+coQRRaSy5+jONGSRCYrxlCwb/rrYY7/xssirugNh\nN/c/EDtmkqSFiFSen0/Zr9IfoLVPV9da2NgUtubR+68RATMmR+V5XqRFq01SK7hYE+5sSj4StjG2\nGVt9SoY/ouiaLdfe+G4dK9zsdJpaR/PZsbMtDKZ7JO3rWoMiqSnkzqvL6kN1lS4OLQxpgz0ZCLu6\nd0d8Lz6LpbQZZTCfHeGEURuDvoyXOtOxa2Tlcbxgb1pRvtGUcaaRU6NhDIvSAS6Tjq7tSIROxOio\nhJFZbjlBHMgOs7W6uXSfAMC//qf/GwDg0/feBwB4lTJuvyJx8B8/YH6EkEBUeR87K9u4cV2qDnlk\npXVKBLfXmcp+TWzB65ev4NpLbwIALIpLPXpfdjnz/kMAwPbmDax2pB/X12UHWfXkGdcYu55lUVHx\nSyt7ZdyBrq2ITf0S0+hf2m4AuXz36cOHz9MtABZCbLZlFcz8aSyErnJYmnDIDw08cujUmlCcK2xU\nUGdhGTuRdrYpKeFX5JlbtgPXl3/HfKfGM/FzjI7Ff7G9tYoV3rNLqYOQ56vS75K2pc8311t4vCfP\ncxYul5hnGLqBgYHBBcELZehrLP11wtVwY9vB9RuMU/1AbKJ2wqzIkdirmpUy2mRPrzKj6v4Js+J4\nzEZTVuc4i5Gk9A6T+WsNUJ+eb9dxYJGhhFz9Mtq2Yghba3pVOL6s1FqlXONUbU+FwchStq8gSh7x\n2OVL0PV6Yj/rlKRTLm2UUGnLTubNV14DAKyWxPZ754543n/y9k/OmBulXQ7t2iXeb41CWuPJAJ/e\nFWa/tyexwFoUQqM4siyFBt6rfdxlmjhD/OFkFsKRtNXT/o7or2DMc8a43/F0F15ZvtvZWZ4zrK/L\nOLFsuV/PRSH4r3UYL+8Iq9xYE7Y7Ho/gMPi7TAbUYiz0G29+DQCKbL7BYCAl5gDM57pbk2u3mIvg\nOE7RT+ojafI7m7uZ4XiKakPGg/p1LJrONfV9wHMgTzGfyTXXXlqy9Bxx/ETGmco+H5+M8PIr0g/f\neENs0//yh/IehTN5ZzLYuLQh79g67c1tZlGvMQfig7el/ONqu4XLG5R0fSJj5fEnEslSscRePp+l\n2L4i8evf/54UePjRuxK9dvhAsoPL4zk2N64DAFo5WTtzNXRH3qHEx0rdLsS59o+WHytFfWIy9CzP\nzjFxYPH8ct2DW2ds70/XG+Uvdbfeadfx+g2531XOQyur0m9j+l9st4yEu9OE4WI1T577V16RHcnV\nV14qaiKH3L1pXodKE+TWCs/fQ5V6AMsy9Bc6oW/QKVT3ZauWbnQw2ZNJq0U98JeusZbnnCFMlosy\nqxpdZsLJk1OZyO894ba7LA/DLeWIqSkc07llW5oowa1xqYzckfPNWYomo4Ms5XYp7fVQqcvi4at+\nMx9eyGILYp5aAAAgAElEQVS5NbapVF+BXxJHSMBJYhk4ulAw5M+J5ri+Lvacm1dFP6Oai/lpnfrT\ntm0hy1UvGWyXmCRG9I05trQvz8q4cvkWAGDnkjjBQia7pOOkOJ+uD6pXoU5XNcVkWV4UnkYkfwtZ\nN1HfgtqaDMwkm8HtSl9es46X7RL4vpp/2E4EyKlT7Tiy4DeaYoY6OZGt6fHJATz2ZWtNxtev/82/\nLX3COptvv/sBAGAymSBVE1fpfDFxdeK5rls4Ts862uQ7eWkzG5gxLFGrz2hY6/aOtCHck0SVcDZH\nlZIJjdrzVeYZs1JXtS7vyG5viE8+Fofnv8/J9ZuvXwcADFjLdB4FWKWZ8CtrMjG5udzj7rsSMjyj\nbIRf9fDgoUzkbYa9UkgTJ4fyHC2/j5Vmm+eRe45YlWpChcLju5+idF1qBb/KsMcKF//1jtz7jTdl\nkXnn3T9DwrHconrhMnBpRtRErizNsZjI1fF5foK3Hbd43ulTuuM5rDNHAoNJgHU+y6ubNJlQC2rI\nCme2u3CQbvLYGgna5duSIFntrOPuXXlWFZcBDVw2yKFgk4CsrG2i0RAyc0r9qmeFMbkYGBgYXBC8\nUIY+0xwTqvyVHAc16iNfpePzGlO5hweqQx7DK8kx5ZqcYH1FjnlwKCaSB3vyeeNSA1GkQjrcivm6\ngjM1N0qR23Qk0gyTMYlowtC44ewE9bkwilpL2LGysmB2Po0896qwWS4oSk6X7pNbX5XzT7tyD/l+\nB5e2hUlptnuvJ99duiSr/7VrV7C7J05kZRpl7iTW6ej6znd+BQCw0twqQjlXqKC3v0/9aUe2eHme\nLVL9yVQGFGBSBuP7ZWxsyPZRw/n2jx8CACZk/COm+d/bPcG163LNibe8YJnuRH7p21Kj1PVT3PtY\ntvWtlqapJ+wbaWe/P0CVpgSXioxTMvMPPhCTwimVKIMgKH6vjuJCCY87kna7jQodzCEV83QMuGT1\n8yRCwCSbevV8strqmvR1SptV/7CHTo066q3lU9wBwGtJck5wKqw/zlIcHohO/L/+4z8EAHz9W5IQ\n9K3XJInlj358v3BwXr0iuzatcP/BQzHFHSXq7M7w43ffBgA0a3JskyHGQyosVmFhhwlac977JqUz\nvvNdeV6f3n2I04esysRx1C7LOxzyM+hLHx4e7aFGJ/HLL7+xdJ9oMEAcL8ZZYXJRkyLHcKGL7jhI\nmHCYPyUdoDstl4EXvd4A738kzLrBKmYtzl/eXM47mx1ji4EHO9fk3dUgjzpF4E67PVg0N9XZtzOV\nAOCYyTnPtVfXscKogicH3aX6wzB0AwMDgwuCF8rQNVjf4srpIkenJStbRnYVsqqLw3CvaX9QSMPO\nKaTUJhtap01+MmUdwEkE212kAANATvatYUmlagUxWZhWKspYbSRXe5ztIqZRjj4uWOwqhw7VmCu8\nZbko0d6e57tL98neoazAfihtefnlL+EWHZTph6IpfropTrT2ijCE3/jPfh3370pIXkifwWXW2lxf\nl88t/mYyOcbgQNhtmbbpDvW0U/oZZkGAOm34WsezWiXLnQoLi6IAL78qzkW1z592JfSvzAo4ynZG\nvQzvrsm5959HPpdt+erXJITO9SP0jlmxiCGNc9quNexwMpujSru65bGSTCR+GYdCUyn7NYqigpkv\nGDrDW88kt7QocBbFDGEla1N/xXw+w4w7NodyFFWOrTASFlxiKn6zXkWLWhUNhrsti6guvpVhIgzd\nsm0loTildPHhLlP0r4tjve7McWtDWGMzlT7bYjLWO11hnn/4J8LK634dNiss7Y5kXKasjdok6e3A\nQbwmtvwWK/lUee9XyWD9zVX88K7Y3CM+t6RO4bamtGF2xDq+cY75VE4+z5ffuSjTPosFQz8fvliE\n5p45Jn1a3KtIQKQVoWLjHsUBV1fl/b78qmjE79FJ3T3cQ63yDfm9r85MBlp0ZYcyGg6w1pFxqfV+\nSdhh8zc2mXqj1cYW/YV37i83pxiGbmBgYHBB8EIZ+ts//lMAQMYkkVathTdvMhGDjOveQ1nRV1l5\nKM4zpFSRajIUz06EObm0k+8dS0RMOI9RqjJKI5MVtsKIFnAVhFuBR/bvUQYzZMq+2tndcg0eq7ho\nmKIWwwBFnU6PJH2+WinBq4hd1+V5l0E24A7CEda2cfQT1LqSOPLxre8BAIYD1gcF63uu1tBpSOXx\nlGxRdyYqYxCy0EEUzPDePWFiXVb9qZSknRkLXaxvbBYJSX3KyM4CCUHT4h/1+go2dqRPbtwUpjgY\nCIv79I48O4fMw7MtDBi+5dWWT3PXAgBKP9c3OvjSl2hfDaQv+kw710pSSZbBoiDUgGnTfWbZrHDX\nMmTCUZalxX0tKuYwSoUiTXGcwLKY8s9ohCmz1TKGac6joAg9GzBZLSHLCik4lRTiTzZadY4pPF+U\ny8SW33e2JELEtx249F9UfWnHmBK5//x//x0AQG9ewa/8DYmWiqfynUcBrm3W3c1CiqxNbdTZ9VVG\nc1VL4uMpx5RXjjJYfWHF+h4ntvTzv/njPwEADIMYU0pHuOSMFU/ON+7LscOHIjM8nOaYM6TR/2T5\nHe6ChKsPKCuKshTJQpTBUMG9JE0+I7urWEgJUArAq2JOG3x/xoQ87ug/+LEUl+n3juHRF/jayxLV\nkvOaKsG8vrEBP+WY5TOzOZdYKhTHa7Y6bdx+Vcb7R3f2l+oPw9ANDAwMLgheKEP/6CNhnlp+6vLW\nJbzKqBaXESeTESNNaMstJxEsJoRUWbyiQ6GfCpNoco0prvhFCbZaTeskysppMzIhzl3krFWZUAQr\nJWOaM0LGyucIafudRVraTe7BV3ZLgaXRdI4qWXGZtvRl0GSygsWY+JJv470KhcpY2d46EbbtpvJ/\nx7GRMnLF46qu8bMJRcS0zmm3N8TbH0q88XAo313aERvqjesSn3771i2U6WF//wOJ1R4M5ZphJDuT\nr379Vfzge38dAFBfYdz9lDHgx/L/0558jqchWhRZqyTLlxVTmqFSuRtr27h+RVjkR+9Jost0Tts+\nBZNcJ8doJLuLjz4Wm/AhpUy3GDXk+Q7/v4mIPosk1oQqGX8qrQwrLoqZqMSuzRwP9b3YFpCzCImK\nh0UzrT2rMcYq+uSgyeiWOF6+6Ifcq5yzXpNdbXUtQc2S98SmfTyidPD9PbH77vVO8H//0e8BAJxr\nwh6vflOef6skbb/FxJsblguX0U4z7jrDgGySUUEtvwyHpNZlFJHTkGP3uWM7GE/hWuq3YGm/VJj5\n/RMZg49PpZ/Ka2soMcnoCWWenw9npHGfltTlzk3/nmVZ8XyKY3nfC5E77ihmIVz6CNY3ZWf64Ucy\nBn/0ExlnSZoiLb0LAHj9ddk5V5oS3RIyoqjkeZirRIijuTEqalblLci1fc/HKhPmVMLjWfFCJ/Td\nJ+LIy1hLz8lsRLE4vkp0FNltmRRHzJCZJxFmE3kxW2Xp2NUd6oNbOolRBa9UR5lbfJcqdD4dbDMm\niWRxhiiX255pYWSablJ2RzAJkE+1tqIMxBGTOlrUcXB8DWmLMedL5NWW1y2xLCZCUf/4vXIbjTo1\nMlg428cqj5b2lUplZA7rg9qaAata4HKfE+p0z6O4mGzUgaeO6L/6q78KAHj9tZcRcBupfVoqS7/5\nZfn/V75+FTVmut2/K1v3g31pX5uJJkd9eSFnYYqIi8/l2sbSfVLso+mYTiNgPNIizprgxc/C+eih\n5tNpGcjEPhnLBHPCcMOrNyWr8tbN2xix0PVopMWnOfHQvFKpVrC+IffVoXbJYCD3naSaXZrj6PCI\nv5e/HexRyZJjsnBEN6solbUa8PKqggAQ0EFp0UQYRjFKroxhR0tsuTRVUiEwyRO8e19Mbjfpi/1r\nDeqesNj1Dgtbb1crCEmO5qyZW+NEee2KONlXq1VgyloDXHiPqEkSRNI/qVtGEJ2vvzqJpO0PmTmb\nMCyvU+8UioJJsrx2vk7Ai0nclsxnLEIaHVuT5WgqyrLiGOTnw5x1ItcJPstyzPkstTbtHkOGNTGx\nVKkWtWnJ/+CWWfBalTVdHxHnIEezzZmg5LK4dnpGv33M+W/CeeeZ+2Opow0MDAwMvrB4oQw9nMuq\nb0OTV07RZ+LBCisCDbuySle17mi9jDii9giZ0ZTbwBorkm+o9nScocHEE09rQ9L04tJJZYcptIiO\nRdNPTKYekd3GaVqo+2V0us3JBDVMsMFtZpIu9MBzt7J0n/hcyLNQ+uR0OEW3J0kclaacr1PXWpbS\nR01/BY0a75PMXB0/amrxuEV2vWrBdNtk5r/yXXG2fvOrX5K/rzQwngu7+sFfET3sWzeEzd5huvKj\n+7v40z/5MwBAjdV5Hu/Ks9NK8Y02nTxzC9ZInl+ps7zJpUgE4U5q2B9hd1dC9TSsUtPyNezMtSys\ns/LO9obs4E76Mt4sX8PB5P63Ll1GZ01+v78nTqfTQ2FdNWpobG6vFSGH3a6Yko6PhI1zZ4w33ngN\ns0DHB9UbJ9Rw55jYoO5HpewXZeOfrlX6rJgwhLRcopKh7eKQ1XSubMtOaFaMZX3+Dvo0Sx2FwiJV\n72jSFSexR0mHWRhhOKXGObne7avXAQA3d8QRa6cZrLqcO+BO+/4DSdwas+JO6HuYZPLOsiwsZnx/\nRnSc12jOmochHDUbWsvzy4JRn1FN1CQ5NeOq9nmqdQuyrEgg0upRi08936K+AP3BOD0VS8HKCmve\nMiAht21sssKaKpjOmXRUq8mYc3wbGcN99dwlJlnZVHZVV3kUJYUZU8/3rDAM3cDAwOCC4MUydAof\nKUPv93vYo3jR1bcYikVbekZnQrlRR3tdUs7DroQ6BVolnCvlGiuuxMEMuYpxUZvZcdVhSaZYAuKZ\nrKwlihSFGe3YGdOH8xx5qkxHVtow0uoidKapY8y2EbE9ZX952+j+A4b8BcreZqixNqk6c06nwgw3\nmSIdpJXChtypiZ23xPtMYhWxYjhkmheM8JWXxGHznV/+SwCAZlvOP4uPkDlyTId9+nv/l1RH+r0/\n/F0AwPaNHDmEYbx8lbUxUybwbLHP6bQO5jnikjDW/P9Dm/rnQUWV1OF4tH+CIROINKFIx5IiTeJi\nMF+9LE6/D++Jz0b9Clq1ql6rYqXNBDEmgDiQfltblTC9JInw7jvi9LpPFcFOh4JtdGZVKjXk3P2c\nnAqjijk+mkyyUZ31TqeJCjXTR4PlJSIAIGZiXcSx59kuBgylW1ExLtp5k1zD73LMmfg1SYWpn55K\neODpkbxPc74Pg/m4cIq+cUMc5q+wwpBGBcRJjHJJ7m0WyO7glPbeQB3EcYogp8+KInZdVucJVdBr\nLM9zGMzRYnWy1or6ip4d6nTO+L4iPxN6WNjOuZtL1Ya+UEVVhl84r9UBrmGLtoOEfXv/wUMAQJ3h\np9s7Mu+4not1ymLs7ct85jL0tkq/mmN7sBl44LiapMgRq1XGyNGH0xEe3JdrzYLldnOGoRsYGBhc\nELxQhp6TEatdKJ57ePBQ7MVv3ZLVbmNT2NWwJ6w0zdKi+oo9lxUup51Q7V1lrpiVerUI7Vpck5Es\nZA+5Wy4qo7tk3R7ZR672M8cukozyhOGLtJ2rDVc95p7jIgiE+czc5dfHuCd9wsAMVH2nCLVsN4Wx\nPPhAVn2QxVdbccEAK2xnzKQJj3Z2ta3nsFEqiS3+lZeFdTk2GdlYGNosO0arLTbA4wMJd/s//+Cf\nAAAePZFU8p3bt3H9muyirt2Sc29sc3dAHed5JjbGJ0cRRrTLWvHyuxZNGJsx8mfvySEsqAgTk3rm\n83P/Tz0HAQXVdraFLb3E9PdTRrQMaQufjwfo7Mgxr1LedLoj93D3nrDx9959FweH0j+ahKRiXTdv\nij59tVJHm/IALm3D9QqlBCgAZ3P81Sp+YXP1lwxFK0B5YPXnpAASJgk9YK3V1VVlu7LTSu0uPIYM\n+vz9bMQw06FEAfU5ljvtGl69LBLLr1+VseKRPk6DRVSVhtsN+3KeEZlv7GoNgQglj7usKjXyT8Q2\nH7ENFdYodTIgZ4hspbp86r8y7IX8bQ6LzFdDE7X6VnbGhl4w86fCFNUWrzZ123EKid5uT8b3O+9J\naG+VkXTXr11GSj+Saq9XKYWr4xNIFrUUNDQ2V/kQ+e9kypDOx49wcnLK+1nu/TEM3cDAwOCC4IUy\n9A5XLYdxyyWvhJQxtY8OZLW/tC6rdOYwzjucwWFdz0pT7Jtj1oPUakQN2uAsy0HAmpOpBrdytS6l\nGrPuINdV01ZWKyumo4JedoiMK3SmiUWMB51MhGkEjJpp1OtFVIbGSC+DKtnzL78kjPPKox4+cIVd\nDV3aGxmVcI81F1v1FrY3JZKjzaQmp6b1DZnmTMbou8D1q8Ksv/yWpIAPhtLXQ6awp5Vj+CV5Jh9/\nIgz1jW/Ks7rxVbG7+2kD168Li7d8YVSrDbn2hLHco7Gcr9zIsFKV3YUmZi0DZRldRhUcH54UFQcW\ncqlyLbWPhrGHPmPV3bmwpetXZNe3s0UJ2JE8p4MnD9BkkYmA1WOOKW71ySfiOzg5PSpEwtY35Hlc\nvSJs/vIl+fS8UlHhqMpauCNofVWOTQrJWVZWMPRGbfloKEDs1wAQ0L+U2h7KdXknBn3WoOR71GaE\n1MrqKvyQxRcYmQXGe+f0+QxYZslxXVxZkWdcog1+MmGcfqKRVjUpIQXgeEyZCFvt44w4cnNcorjU\nY7LOhPHnEQXOtEBLybMASiyEzxX9c56hW4s/IWWkV6bx51oMNgccV5P0aDMvmLp97v9Jli4oNAfm\nYChzwHgi99SoeNhel/fHc6T/6vSXaIJjmszgNeVvWo1I661Oed8H+7IjPDrYQ8LkM60Z/Kx4oRN6\npcoQQjoFSr5fOHEeHsgWw6f+skOnShylqLAcWsqqRhqGlrFKSGOVWVkxkLhMEOGAnGvlInaeM0+Q\nBHR0qg50EcIkD9f3/CJUKWZYWkBTSzCleYUDNE5SdKjyly0/d+F1bs2/F8mLuLtewRNu35KRJKm4\nV6Rd03fk5Zr1p2hRUxkOXwy+nCkHr9OQybCzHePLX5NtdKVJ3REW6Y6ZEJU3xuhN/ggA8BEdgK+y\nokyPDtkHPx3DdqX/PTqQoom0fUATmlaFalQ9pMxgTBb+p2eGvoCnR9Ink/GkyNp0+QI+rZY3mQUY\nMWTQo26Iz0Sjy1fkXmbUF3FcF6cMQfzJT38MAGivyBjySQA211exsSV/u3lLQjjXWAnJJdmolBvQ\nYLOIk0fKyXJlTcbEKrNDwyQpkkTqz2FaAICEi6PF5xbFEWwsMlEBYMKww5QO/kajhkZNFtcJn8Vw\nKIvXzU1ZDB425Fm5FuDzvelyjIxJXEq8z2q9jSnDh094nhLflQrfNb9RKioe3TmR86iDW7ORMppG\nE8TIOQeoTvsyWDjdOSE7VmEO1XdBAxiyM6RAJ2yd0C3o+DpfLB1ZDtfVRRnnjtHJfzCawuIxqm0+\nJvFTx2e1VkXOJMI5EwYTTVTalZDZ/on0Z5bEGHMhTH+GmuTPgzG5GBgYGFwQvFCGPmLqclrQtrxI\nKjg6FLYdBbKdffNlSTXO7LTY4rVY7BZkCI0NYZ4JE3q8so/ME5Y4HVI6gMuypu7P4wQ9fre3Lyuj\ny/TsOrfPSRhhxFTe0UAYxnzGIrlMNFJtl95gjPm6mHw6nebSffJ11hf8YSJb1H/jNHDKNPK1kpg9\nbGq6NGpy7De/9xYmPbmfKXcMCcPCSj7T0snmWmsp6ivC5mOyyCSWe+qyXmFyOkS5Lfe3c1XY3HQm\nfbTaETa5+f0a6iqKnUp/nfQOeD758+oW9cidGL2Rz2suX7Foym3+yaEw9BwLpqLbZnWKBkX4olOo\n9rU7wjxzbRh3rR0moh2dnOLOJ58AWOjbuK58Xt6WMXb71etok2VrqKrqwPtU6YzmeZFYFNDMo/Uh\ny9SN8TWBJk4Q8tg8fo5tCxZFuZ2i7mVeyGiU6eifz2kqIIsfT8NCluAyzQCnHNPbrCews0qZh9CG\nzfdlTFmAiM7LDvWPSmUX93uSjNWdyXnqDCZI6ZgPkwRz/n7G5xap3AGPsbRwc5Ii5TXieHmTpVbU\nWujYW0jJfPU7VVTU9H7bsc/p3p/97mkpdcd2PuM4LaqhuVq3NcPb737C25N+unlLnMoTvp/zcA7Q\nOR1QlmM8paY/NdMjBl4cnQyK6mlPt/PPg2HoBgYGBhcEL5Shq4OlqDKSL4JyBlxF1dmo9q7rV3ag\nHrEq091BMZtxJL+OesImPdctGIDKHGuVoxGvPZrOMBzq8RQp4qfatOZhhMlQVs0xbYkjrrTdgfx2\noOJO0wkePJTfv/7a7WW7BP+yLbuMnoZFDqYo18RpMkiEoVoB08cZEnd0tIcGbXUuq/E016kex/tN\nVHN+M4KVCROb78n6HWcU/onERn94NMAGbZwNT5kKQ6uoSthYLRc7rIz9FKdyTLOjGuLy7IJphkB9\nF97yQyyisJM6Pl3XLRwU9tNCS1qFKImLBKqS1vxkQtHRiYRidh/JjqLX6xchbJ012ZFsbEoY4ze+\nJUlX7bUmJoEwKY8syaGIla81ZMMp5kwq04QuDW2s1rQqEW21OdCinEXZfb7XTuueRtEieUzrnFpM\nqNMQX/U1xEmCKNSdi+wCLSbdpfrMW9xZda1CXz6mDRfcvZYoepe6OY5G4u+aUUmxygS4FoMURmmI\nlMEJTe4CQDafqRgWr52dYaBaUep5YBVCfUkROpgVVcWeClE887ezKf7AZysXncVnHah6Dhdjhtj+\n+MdSS3WPapc3boqlYX19rfDtzGbSB8dd6b/dRyp2J+9jtzdCMW6WHCuGoRsYGBhcELxQhj5lZEia\nLhh6Gp+X2RyNmOLNY+dRitdfkgSRgNXLC13jobBuj8X50jguwtvmZLwaNjeiPOosiAr7ntYbtfg5\n4zHTSYAhIyaOaN/ap1hOvz/ivWhFmrTYZXzEkL9lcMhQOq8iZ2l22gCjKCaetCEYS3v3H0tbDveP\n8Gv/wWsARBMbAGJfwtZWmLqeMs07zFNUaceNc+qVj+Q+J1rZZ9CHy7Tk1JdraQjgdi52Z9e1EWoS\nDx9fQJt1RJbjUGQoTueYzoW1ra4uL86lNvT0XLX288xcoccE83nB0HMteMRjlLXOGDllO1ZRV/Xq\nVWFQW9S69rijCMMQTiF8pjK+Kvsg5xmOBghm0labCTOdtvR/nSGCGq1Qcn34ZNOaULIsVlelzUeM\n0ImiaBFRx+urXVd9K1mWocmomhYr0Jcp+ZCOGYlRYvJcGmLCHUdMpl4i6/aZXJOGIQZ9sm0yYZvM\nWgXhHNgI+7KLuEo/xG5F+mF3wtBEhnWelYZ4HpkINXYXiYBZVvhZVIBLI6SKvnLsgmXbhUxudvZ0\niwQjyz5Txch66vMsU6e1gO/I/oH457o9eS8rlUpRFUzrEWsd2BnnmpA7rzS3iueYLblrMQzdwMDA\n4ILghTJ0W+M3bU0GOGOP0sWZi1/Ale6T+w/h+azQflMiYBZl/8hGAmWcU8SadEEbckiGfkxmvXfc\nLWKGVVip3RQbdcJV+uj4FPsHYm89ZRKTxhBrOrGntsvcKpjFkCJFyyC3KNPLJ1EpN9A9ENZXXRO2\nl+VyTKfFZKIVwGaRE68qP/TIfDrJnXPt3MM65g5rNjYoE9DVOF3xyM96NspXGAlDVvP4kdgA2xts\nWFpFg3Vej45lF0W1VHSP5jwfbex5gFpZEi0Ok+UE+gFgQElljZCxrIXtVRnL04UN4iTBhDuvCXd3\nyqw1RvvSjvTn2lobrZaKJsl3+7sP5b4f3wMAvPHWG1hZF0bMTQAC5kFMyboPDnYLSegWRbhaLWFh\nWqygT5/LSqtV2FCD54i3Bhb2eW37aDTCjJK6GspTxOcXO5cAZcZIg/kcTUaHnTKS65Tx5OHUwqQl\nu2GbtnOf8eQW38HRaIxT7qCmym7JOBskk1Xbx1ZTYvY7tPtPm3JsdyQyFnO+8LZlFc/2eZAUNXVZ\ndzdLil2cUwhuqa178fm0ZECWnx9XsM7/5ty/NZRJxyAW+RC6WwGrHKksdhQnRdEK3Q1EGoVVnJY2\n+jxDyvZYznKc2zB0AwMDgwuCF8rQZ7TZLer3OYWUbgH9L1fO09NT/PQ9ph1TED5LNeWW0rhkPkdH\nx5hSljNjVIRWat9n1uFRb1Bk3OnFKowYUebeG44wo5dem6M2LQvnRXzsNC2E85/HBhgx/pmBIyhX\n5vAbZC+Md7ZLtLMzbfrmrSvorMm/2zXpm5UR+/ZI2O20LGzUbXoIaA/1WK6sxnjkeiCfnlXG8ITV\n2CesX+qr7XthE7xFsTC1VbtkfD77ZDBhsYd5hMFAYpWrz2FD175XG6jjOECmfSBMccGwFjbGCUvs\nTcnUWxQw00iUHkvIra2tIZ0z81cjRmjXfvL4kdx1OMHLr4jsgcNIoj0Kl03JiofDQWG7rlMmVhlj\nn5ElKe2s4yBApSFjKHlOG7oyOB2LtVptEflCn5FGwGjqebPZRIPp5+tbUlv19utvSZv25V6VTc4s\nIOKM0GTcuUdJ5JjXOYoiPKYT5VOLJdXYvhuM7LlaaWDt0nUAQIf+rcSXyJgHzBD9iLZly7ZhZc8f\n3aL37Wb0C8TJZ9j106bvs989XX/06e8ty1qwdlg/87dZnhe7DLWPZ9BoHvltattF5JGOa43IW5gn\ndNfiwKacgkp4PCte6IQ+5qSjN+DaNpzCgaWp3FpXT2v92RgwhPDN12QgamD+Xb6gtbK85IfHx8U1\ntIpOFMrnZKIB/tEZBTR9mPK/hNulHIDHCa1IEdaHWiQesH1uiiRNzv1tGTT4EoxOpH3paor2hiq1\nSdtrddn2jlwxA115/VWstlmsei4L1bRB7fSUWuVjviTRHAHDO3tjVjNyZbKpc9K/cf0GUl8mqyd7\n8uLdelXasLYuZpaDozHu3aepJZS+qFSp3kc9lNSRe8gtC+MxU883q0v3iRaHVk3pcqmECV/cp8Xn\nZi32x3IAAAOUSURBVGf0PwrtDo4hTzXx+eKMWUQ6j0P4nIB1q60FpH2Sjf3Hj3BIbeuEekMzhlOq\ng7DeqBfhoWGon9IHqtBYFJEOQ4SxtKdU9pfpjgIDqkWedru8ViihugAc9pXvaR1Nv7g/JVJaC/PD\nhxKi+Xgok+2EYcAnyQjvUqu9YVE/KZ/zfsQR+nGvi5/OpB93+Z2qY45pXuunDk5O5Vo2NVsC6sW0\n6Nh1+E6nSbJQSnweQkTH/FlTnIYXLuwP5ydi214kFqn5o5AA0BBVR99754y++vmJ/Kx5Rif0ovg0\n12xtSppkiEiOFgsEv1MJDvXmwyr02LUS2bPCmFwMDAwMLgheKEOvcuun1UHKnlOshBlXqbJWwmZy\nSJJGKPFvPp1CI+o4q8NzSCfaSbeLQLXW6XBIyMYXWsh5sXVd0D0yusLZahf1CIutV26dORLI6YRy\n4MDNtRuX1/72XQlvsxy2e2AVGsjVBuus0gyyuU12480QjoRBZQdaQ5TSBBVuxxmamPRzxGRt0xlV\n8mJhUt+++nUAwOVXVoFUQuEadany04tlOz7UFPA8w/4jYW9r69RldySkMUt5fqYrl9wq1tepMhcs\nz7rUoTdjIpPr15HEmiQi1/bLTI7hEylVygWzD+ggtldUeElgWyoKV0XZk7E0G1IrnONttUNt9/AQ\nA+p0dzm+9ForPAZ5iDkd8ir0NuQuyHXPK/1ttOvFjiEvjBTL4ctvSqjqIeufnmW0C/bJ8XomASvn\n7kFLBfzx+5LAMp1T2uCSqHA2GyF6lM4YFuJVdPzx/8etVexsfQUAsKnvC9vgaq1Mx8XkKU1yZbeb\nfK++S836NE0LltyhZMMy0B7Q99sr+UVW4kKA63wSkeu4xe8K04a+58rM2X7Hds7JCpw9T/HXHMU1\n1dSiVbeK2qVnfqDjW/9QOPzZD1mWF79bdtdiGLqBgYHBBYH1XMH8BgYGBgZfOBiGbmBgYHBBYCZ0\nAwMDgwsCM6EbGBgYXBCYCd3AwMDggsBM6AYGBgYXBGZCNzAwMLggMBO6gYGBwQWBmdANDAwMLgjM\nhG5gYGBwQWAmdAMDA4MLAjOhGxgYGFwQmAndwMDA4ILATOgGBgYGFwRmQjcwMDC4IDATuoGBgcEF\ngZnQDQwMDC4IzIRuYGBgcEFgJnQDAwODCwIzoRsYGBhcEJgJ3cDAwOCCwEzoBgYGBhcEZkI3MDAw\nuCAwE7qBgYHBBcH/CzblIxZ1U0R/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2b33d6a090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualizar_dados_treino(num_dados):\n",
    "    '''Essa função apresenta alguns dados de treino (imagens, labels) escolhidos aleatoriamente.\n",
    "       Parâmetros:\n",
    "           num_dados (int): número de dados que serão apresentados \n",
    "    '''\n",
    "\n",
    "    print('Exemplos de %d imagens de teste' % num_dados)\n",
    "    \n",
    "    # Escolhe índices aleatórios\n",
    "    random_indices = np.random.randint(0, dados_treino['imagens'].shape[0], num_dados)\n",
    "    imagens = dados_treino['imagens'][random_indices]\n",
    "    labels = dados_treino['labels'][random_indices]\n",
    "    for index, (img, label) in enumerate(zip(imagens, labels)):\n",
    "        plt.subplot(2, num_dados, index + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        plt.title('%i' % label)\n",
    "    plt.show()\n",
    "\n",
    "visualizar_dados_treino(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2b30123850>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'models', '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into models/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.680092, step = 1\n",
      "INFO:tensorflow:global_step/sec: 8.95124\n",
      "INFO:tensorflow:loss = 0.685584, step = 101 (11.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.02329\n",
      "INFO:tensorflow:loss = 0.714652, step = 201 (11.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.33689\n",
      "INFO:tensorflow:loss = 0.672315, step = 301 (10.710 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 313 into models/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.615244.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-02-16:48:22\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-313\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-02-16:48:25\n",
      "INFO:tensorflow:Saving dict for global step 313: accuracy = 0.645, global_step = 313, loss = 0.634329\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-313\n",
      "INFO:tensorflow:Saving checkpoints for 314 into models/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.622818, step = 314\n",
      "INFO:tensorflow:global_step/sec: 8.63104\n",
      "INFO:tensorflow:loss = 0.640421, step = 414 (11.588 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.45887\n",
      "INFO:tensorflow:loss = 0.727841, step = 514 (11.822 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.16422\n",
      "INFO:tensorflow:loss = 0.667839, step = 614 (12.248 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 626 into models/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.631067.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-02-16:49:05\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-626\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-02-16:49:08\n",
      "INFO:tensorflow:Saving dict for global step 626: accuracy = 0.6745, global_step = 626, loss = 0.595821\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-626\n",
      "INFO:tensorflow:Saving checkpoints for 627 into models/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.651073, step = 627\n",
      "INFO:tensorflow:global_step/sec: 8.46154\n",
      "INFO:tensorflow:loss = 0.516572, step = 727 (11.820 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.78527\n",
      "INFO:tensorflow:loss = 0.62469, step = 827 (11.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.33326\n",
      "INFO:tensorflow:loss = 0.752998, step = 927 (10.714 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 939 into models/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.581463.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-02-16:49:46\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-939\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-02-16:49:49\n",
      "INFO:tensorflow:Saving dict for global step 939: accuracy = 0.69, global_step = 939, loss = 0.565844\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-939\n",
      "INFO:tensorflow:Saving checkpoints for 940 into models/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.50283, step = 940\n",
      "INFO:tensorflow:global_step/sec: 9.07048\n",
      "INFO:tensorflow:loss = 0.673204, step = 1040 (11.026 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.04084\n",
      "INFO:tensorflow:loss = 0.514022, step = 1140 (11.061 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.2674\n",
      "INFO:tensorflow:loss = 0.453821, step = 1240 (10.791 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1252 into models/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.443007.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-02-16:50:26\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-1252\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-02-16:50:29\n",
      "INFO:tensorflow:Saving dict for global step 1252: accuracy = 0.702, global_step = 1252, loss = 0.55074\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-1252\n",
      "INFO:tensorflow:Saving checkpoints for 1253 into models/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.569787, step = 1253\n",
      "INFO:tensorflow:global_step/sec: 8.31575\n",
      "INFO:tensorflow:loss = 0.601642, step = 1353 (12.027 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.07604\n",
      "INFO:tensorflow:loss = 0.441335, step = 1453 (12.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.5816\n",
      "INFO:tensorflow:loss = 0.531486, step = 1553 (11.654 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1565 into models/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.497967.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-02-16:51:13\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-1565\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-02-16:51:17\n",
      "INFO:tensorflow:Saving dict for global step 1565: accuracy = 0.7285, global_step = 1565, loss = 0.529409\n"
     ]
    }
   ],
   "source": [
    "# Checkpoints e logs serão salvos nessa paste, se quiser treinar do zero\n",
    "# delete a pasta ou tudo que tem nela.\n",
    "MODEL_DIR = 'models'\n",
    "\n",
    "# Hiper parâmetros\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "TRAIN_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def model_fn(features, labels, mode):\n",
    "  \"\"\"Defines the CNN model that runs on the data.\n",
    "\n",
    "  The model we run is 3 convolutional layers followed by 1 fully connected\n",
    "  layer before the output. This is much simpler than most CNN models and is\n",
    "  designed to run decently on CPU. With a GPU, it is possible to scale to\n",
    "  more layers and more filters per layer.\n",
    "\n",
    "  Args:\n",
    "      features: batch_size x 32 x 32 x 3 uint8 images\n",
    "      labels: batch_size x 1 uint8 labels (0 or 1)\n",
    "      mode: TRAIN, EVAL, or PREDICT\n",
    "\n",
    "  Returns:\n",
    "      EstimatorSpec which defines the model to run\n",
    "  \"\"\"\n",
    "\n",
    "  # Preprocess the features by converting to floats in [-0.5, 0.5]\n",
    "  features = tf.cast(features, tf.float32)\n",
    "  features = (features / 255.0) - 1.0\n",
    "\n",
    "  # Define the CNN network\n",
    "  # conv1: 32 x 32 x 3 -> 32 x 32 x 16\n",
    "  net = tf.layers.conv2d(\n",
    "      inputs=features,\n",
    "      filters=16,                 # 16 channels after conv\n",
    "      kernel_size=3,              # 3x3 conv kernel\n",
    "      padding='same',             # Output tensor is same shape\n",
    "      activation=tf.nn.relu)      # ReLU activation\n",
    "\n",
    "  # pool1: 32 x 32 x 16 -> 16 x 16 x 16\n",
    "  net = tf.layers.max_pooling2d(\n",
    "      inputs=net,\n",
    "      pool_size=2,\n",
    "      strides=2)                  # Downsample 2x\n",
    "\n",
    "  # conv2: 16 x 16 x 16 -> 16 x 16 x 32\n",
    "  net = tf.layers.conv2d(\n",
    "      inputs=net,\n",
    "      filters=32,\n",
    "      kernel_size=3,\n",
    "      padding='same',\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # pool2: 16 x 16 x 32 -> 8 x 8 x 32\n",
    "  net = tf.layers.max_pooling2d(\n",
    "      inputs=net,\n",
    "      pool_size=2,\n",
    "      strides=2)\n",
    "\n",
    "  # conv3: 8 x 8 x 32 -> 8 x 8 x 64\n",
    "  net = tf.layers.conv2d(\n",
    "      inputs=net,\n",
    "      filters=64,\n",
    "      kernel_size=3,\n",
    "      padding='same',\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # flat: 8 x 8 x 64 -> 4096\n",
    "  net = tf.contrib.layers.flatten(net)\n",
    "\n",
    "  # fc4: 4096 -> 1000\n",
    "  net = tf.layers.dense(\n",
    "      inputs=net,\n",
    "      units=1000,\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # output: 1000 -> 2\n",
    "  logits = tf.layers.dense(\n",
    "      inputs=net,\n",
    "      units=2)\n",
    "\n",
    "  # Softmax for probabilities\n",
    "  probabilities = tf.nn.softmax(logits)\n",
    "\n",
    "  predictions = tf.argmax(\n",
    "      input=logits,\n",
    "      axis=1,\n",
    "      output_type=tf.int32)\n",
    "\n",
    "  # Return maximum prediction if we're running PREDICT\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions={\n",
    "            'prediction': predictions,\n",
    "            'probability': probabilities})\n",
    "\n",
    "  # Loss function and optimizer for training\n",
    "  loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=tf.one_hot(labels, depth=2),\n",
    "      logits=logits)\n",
    "\n",
    "  train_op = tf.train.MomentumOptimizer(\n",
    "      LEARNING_RATE, MOMENTUM).minimize(\n",
    "          loss=loss,\n",
    "          global_step=tf.train.get_global_step())\n",
    "\n",
    "  # Accuracy for evaluation\n",
    "  eval_metric_ops = {\n",
    "      'accuracy': tf.metrics.accuracy(\n",
    "          labels=labels,\n",
    "          predictions=predictions)}\n",
    "\n",
    "  # EVAL uses loss and eval_metric_ops, TRAIN uses loss and train_op\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      loss=loss,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "def input_fn_wrapper(is_training, dados):\n",
    "  \"\"\"Input function wrapper for training and eval.\n",
    "\n",
    "  A wrapper funcution is used because we want to have slightly different\n",
    "  behavior for the dataset during training (shuffle and loop data) and\n",
    "  evaluation (don't shuffle and run exactly once).\n",
    "\n",
    "  Args:\n",
    "      is_training: bool for if the model is training\n",
    "\n",
    "  Returns:\n",
    "      function with signature () -> features, labels\n",
    "      where features and labels are the same shapes expected by model_fn\n",
    "  \"\"\"\n",
    "  def input_fn():\n",
    "    np_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'x': dados['imagens']},\n",
    "        y=dados['labels'],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=is_training,\n",
    "        num_epochs=None if is_training else 1)\n",
    "\n",
    "    features_dict, labels = np_input_fn()\n",
    "    # Since the only feature is the image itself, return the image directly\n",
    "    # instead of the features dict\n",
    "    return features_dict['x'], labels\n",
    "\n",
    "  return input_fn\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Create the estimator object that is used by train, evaluate, and predict\n",
    "# Note that model_fn is not called until the first usage of the model.\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=tf.estimator.RunConfig().replace(\n",
    "      model_dir=MODEL_DIR))\n",
    "\n",
    "steps_per_epoch = dados_treino['imagens'].shape[0] / BATCH_SIZE\n",
    "\n",
    "for epoch in xrange(TRAIN_EPOCHS):\n",
    "  estimator.train(\n",
    "      input_fn=input_fn_wrapper(True, dados_treino),\n",
    "      steps=steps_per_epoch)\n",
    "  # Evaluating on the same dataset as training for simplicity, normally\n",
    "  # this is a very bad idea since you are not testing how well your\n",
    "  # model generalizes to unseen data.\n",
    "  estimator.evaluate(input_fn=input_fn_wrapper(False, dados_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-1565\n",
      "Probability of cat: 0.00412\tProbability of dog: 0.99588\n",
      "Prediction DOG\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_image(image_file):\n",
    "  \"\"\"Convert PIL Image to a format that the network can accept.\n",
    "\n",
    "  Operations performed:\n",
    "      - Load image file\n",
    "      - Central crop square\n",
    "      - Resize to 32 x 32\n",
    "      - Convert to numpy array\n",
    "\n",
    "  Args:\n",
    "      image_file: str file name of image\n",
    "\n",
    "  Returns:\n",
    "      numpy.array image which shape [1, 32, 32, 3]\n",
    "\n",
    "  Assumes that image is RGB and at least 32 x 32.\n",
    "  \"\"\"\n",
    "  image = Image.open(image_file)\n",
    "  width, height = image.size\n",
    "\n",
    "  min_dim = min(width, height)\n",
    "  left = (width - min_dim) / 2\n",
    "  top = (height - min_dim) / 2\n",
    "  right = (width + min_dim) / 2\n",
    "  bottom = (height + min_dim) / 2\n",
    "\n",
    "  image = image.crop((left, top, right, bottom))\n",
    "  image = image.resize((32, 32), resample=Image.BILINEAR)\n",
    "  image = np.asarray(image, dtype=np.uint8)\n",
    "  image = np.reshape(image, [1, 32, 32, 3])\n",
    "\n",
    "  return image\n",
    "\n",
    "image = process_image('dog.jpg')\n",
    "# Define a new input function for prediction which outputs a single image\n",
    "def predict_input_fn():\n",
    "  np_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={'x': image},\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "  features_dict = np_input_fn()\n",
    "  return features_dict['x']\n",
    "\n",
    "pred_dict = estimator.predict(\n",
    "    input_fn=predict_input_fn).next()\n",
    "\n",
    "print ('Probability of cat: %.5f\\tProbability of dog: %.5f' % (pred_dict['probability'][1], pred_dict['probability'][0]))\n",
    "print ('Prediction %s' % ('CAT' if pred_dict['prediction'] == 1 else 'DOG'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
